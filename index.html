<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>3D Head Tracking STL Viewer</title>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; font-family: sans-serif; }
        #canvas-container { width: 100vw; height: 100vh; position: absolute; top: 0; left: 0; z-index: 1; }
        
        /* Webcam debug view (small, bottom left) */
        #webcam-container {
            position: absolute;
            bottom: 20px;
            left: 20px;
            width: 160px;
            height: 120px;
            z-index: 2;
            border: 2px solid white;
            border-radius: 8px;
            overflow: hidden;
            background: #222;
        }
        #webcam-video { width: 100%; height: 100%; object-fit: cover; transform: scaleX(-1); /* Mirror the preview */ }
        
        #loading {
            position: absolute; top: 50%; left: 50%; transform: translate(-50%, -50%);
            color: white; z-index: 3; pointer-events: none;
        }
    </style>
</head>
<body>

    <div id="loading">Loading AI Models...</div>
    <div id="webcam-container">
        <video id="webcam-video" autoplay playsinline muted></video>
    </div>
    <div id="canvas-container"></div>

    <script type="importmap">
        {
            "imports": {
                "three": "https://unpkg.com/three@0.160.0/build/three.module.js",
                "three/addons/": "https://unpkg.com/three@0.160.0/examples/jsm/",
                "@mediapipe/tasks-vision": "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/+esm"
            }
        }
    </script>

    <script type="module">
        import * as THREE from 'three';
        import { STLLoader } from 'three/addons/loaders/STLLoader.js';
        import { FilesetResolver, FaceLandmarker } from '@mediapipe/tasks-vision';

        // --- 1. CONFIGURATION ---
        const SENSITIVITY_X = 5.0; // Multiplier for Left/Right movement
        const SENSITIVITY_Y = 3.0; // Multiplier for Up/Down movement
        const DEPTH_FACTOR = 10.0; // How much Z-depth changes based on closeness
        const SMOOTHING = 0.1;     // 0.0 to 1.0 (Lower = smoother but more lag)

        // --- 2. THREE.JS SETUP ---
        const scene = new THREE.Scene();
        scene.background = new THREE.Color(0x202020);
        
        // Fog helps with depth perception
        scene.fog = new THREE.Fog(0x202020, 10, 50);

        const camera = new THREE.PerspectiveCamera(50, window.innerWidth / window.innerHeight, 0.1, 1000);
        const renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        document.getElementById('canvas-container').appendChild(renderer.domElement);

        // Lights
        const ambientLight = new THREE.AmbientLight(0xffffff, 0.6);
        scene.add(ambientLight);
        const dirLight = new THREE.DirectionalLight(0xffffff, 2);
        dirLight.position.set(5, 10, 7);
        scene.add(dirLight);

        // Helper Grid
        const gridHelper = new THREE.GridHelper(20, 20, 0x444444, 0x444444);
        scene.add(gridHelper);

        // --- 3. STL LOADER ---
        // We will try to load 'model.stl'. If it fails, we show a fallback cube.
        const loader = new STLLoader();
        loader.load(
            './model.stl', 
            function (geometry) {
                // Center the geometry
                geometry.center(); 
                
                // Scale normalization (Fit within a unit cube roughly)
                geometry.computeBoundingBox();
                const maxDim = Math.max(
                    geometry.boundingBox.max.x - geometry.boundingBox.min.x,
                    geometry.boundingBox.max.y - geometry.boundingBox.min.y,
                    geometry.boundingBox.max.z - geometry.boundingBox.min.z
                );
                const scale = 5 / maxDim; // Scale to be roughly size 5
                
                const material = new THREE.MeshPhysicalMaterial({ 
                    color: 0x00ffcc, 
                    metalness: 0.2, 
                    roughness: 0.1,
                    clearcoat: 1.0
                });
                const mesh = new THREE.Mesh(geometry, material);
                mesh.scale.set(scale, scale, scale);
                mesh.rotation.x = -Math.PI / 2; // STL often comes in rotated
                scene.add(mesh);
            },
            (xhr) => { console.log((xhr.loaded / xhr.total * 100) + '% loaded'); },
            (error) => {
                console.warn("No 'model.stl' found. Loading fallback Cube.");
                const geometry = new THREE.BoxGeometry(3, 3, 3);
                const material = new THREE.MeshNormalMaterial();
                const cube = new THREE.Mesh(geometry, material);
                scene.add(cube);
            }
        );

        // --- 4. MEDIAPIPE FACE TRACKING SETUP ---
        const video = document.getElementById('webcam-video');
        let faceLandmarker;
        let lastVideoTime = -1;
        let headPos = new THREE.Vector3(0, 0, 10); // Target position
        let currentPos = new THREE.Vector3(0, 0, 10); // Current smoothed position

        async function initVision() {
            const vision = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
            );
            faceLandmarker = await FaceLandmarker.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
                    delegate: "GPU"
                },
                outputFaceBlendshapes: false,
                runningMode: "VIDEO",
                numFaces: 1
            });
            document.getElementById('loading').style.display = 'none';
            startWebcam();
        }

        function startWebcam() {
            navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
                video.srcObject = stream;
                video.addEventListener("loadeddata", predictWebcam);
            });
        }

        // --- 5. THE LOOP ---
        async function predictWebcam() {
            let startTimeMs = performance.now();

            if (video.currentTime !== lastVideoTime) {
                lastVideoTime = video.currentTime;
                const results = faceLandmarker.detectForVideo(video, startTimeMs);

                if (results.faceLandmarks.length > 0) {
                    const landmarks = results.faceLandmarks[0];

                    // --- PINHOLE MODEL CALCULATION ---
                    
                    // 1. Find center of face (nose tip is approx index 1)
                    const nose = landmarks[1]; 
                    
                    // 2. Calculate approximate depth (Z)
                    // We measure distance between left eye outer (33) and right eye outer (263)
                    const leftEye = landmarks[33];
                    const rightEye = landmarks[263];
                    const dx = leftEye.x - rightEye.x;
                    const dy = leftEye.y - rightEye.y;
                    const eyeDist = Math.sqrt(dx*dx + dy*dy);
                    
                    // "0.15" is a magic number based on typical eye distance in frame at ~50cm away
                    // If eyeDist is SMALL, user is FAR (Z increases)
                    // If eyeDist is LARGE, user is CLOSE (Z decreases)
                    const estimatedZ = (0.15 / eyeDist) * 10; 

                    // 3. Map X and Y to 3D space
                    // MediaPipe coords: x (0 to 1), y (0 to 1). 
                    // We shift center to (0.5, 0.5) -> (-0.5 to 0.5)
                    // Note: We FLIP X because webcam is mirrored for the user, but we want 
                    // moving head left to move camera left.
                    const rawX = (nose.x - 0.5) * -1; 
                    const rawY = (nose.y - 0.5) * -1; // Invert Y because screen Y is top-down

                    headPos.x = rawX * SENSITIVITY_X * estimatedZ; 
                    headPos.y = rawY * SENSITIVITY_Y * estimatedZ;
                    headPos.z = estimatedZ * 1.5; // Scale Z slightly
                }
            }

            // Smooth Interpolation (Lerp) to prevent jitter
            currentPos.lerp(headPos, SMOOTHING);

            // Update Camera
            camera.position.copy(currentPos);
            camera.lookAt(0, 0, 0); // Always look at origin

            renderer.render(scene, camera);
            window.requestAnimationFrame(predictWebcam);
        }

        // Handle Resize
        window.addEventListener('resize', () => {
            camera.aspect = window.innerWidth / window.innerHeight;
            camera.updateProjectionMatrix();
            renderer.setSize(window.innerWidth, window.innerHeight);
        });

        initVision();
    </script>
</body>
</html>
